{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_complete.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1GSrxIMSJO0u8GUF1ZdkU-_uoZ2vGHhx9",
      "authorship_tag": "ABX9TyMPJph/GLPONQkXUp0Uo39p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirhoseink94/Mel-GaN-VC-Style/blob/main/main_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8N0PHLr6l0w"
      },
      "source": [
        "# Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjBTo9Hm6dxr",
        "outputId": "b118869c-adb4-4d85-ab5c-45b7ee5967fe"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow as tf\n",
        "!pip install soundfile                    #to save wav files\n",
        "!pip install --no-deps torchaudio==0.5.0\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import os\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#auth.authenticate_user()\n",
        "#gauth = GoogleAuth()\n",
        "#gauth.credentials = GoogleCredentials.get_application_default()\n",
        "#drive = GoogleDrive(gauth)\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "\n",
        "####including\n",
        "\n",
        "from glob import glob\n",
        "import scipy\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Concatenate, Conv2D, Conv2DTranspose, GlobalAveragePooling2D, UpSampling2D, LeakyReLU, ReLU, Add, Multiply, Lambda, Dot, BatchNormalization, Activation, ZeroPadding2D, Cropping2D, Cropping1D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import TruncatedNormal, he_normal\n",
        "import tensorflow.keras.backend as K\n",
        "import datetime\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "import imageio\n",
        "import librosa\n",
        "import librosa.display\n",
        "from librosa.feature import melspectrogram\n",
        "import os\n",
        "import time\n",
        "import IPython\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "import math\n",
        "import heapq\n",
        "from torchaudio.transforms import MelScale, Spectrogram\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n",
            "Collecting torchaudio==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/7d/8e01e21175dd2c9bb1b7e014e0c56cdd02618e2db5bebb4f52f6fdf253cb/torchaudio-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 13.8MB/s \n",
            "\u001b[?25hInstalling collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.5.0\n",
            "Collecting mitdeeplearning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/3b/b9174b68dc10832356d02a2d83a64b43a24f1762c172754407d22fc8f960/mitdeeplearning-0.1.2.tar.gz (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (4.41.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->mitdeeplearning) (0.16.0)\n",
            "Building wheels for collected packages: mitdeeplearning\n",
            "  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mitdeeplearning: filename=mitdeeplearning-0.1.2-cp36-none-any.whl size=2114585 sha256=5c0dbcbddf695b7ded142178013b71d93d4b052c4e402e36c949e3c6a8899869\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/e1/73/5f01c787621d8a3c857f59876c79e304b9b64db9ff5bd61b74\n",
            "Successfully built mitdeeplearning\n",
            "Installing collected packages: mitdeeplearning\n",
            "Successfully installed mitdeeplearning-0.1.2\n",
            "/content\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USz5Pt4v6H_0"
      },
      "source": [
        "# hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfFqkdT357Ut"
      },
      "source": [
        "#Hyperparameters\n",
        "\n",
        "hop=192               #hop size (window size = 6*hop)\n",
        "sr=16000              #sampling rate\n",
        "min_level_db=-100     #reference values to normalize data\n",
        "ref_level_db=20\n",
        "\n",
        "shape=24              #length of time axis of split specrograms to feed to generator\n",
        "vec_len=128           #length of vector generated by siamese vector\n",
        "bs = 128               #batch size\n",
        "delta = 2.            #constant for siamese loss\n",
        "tag='HAP'             #the tag for the training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF2tOQrX6xkM"
      },
      "source": [
        "#helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lYdEAqi63wl",
        "outputId": "a8f2ef08-b781-46f9-be49-8a2e5986aaa1"
      },
      "source": [
        "\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "#MEL-SPECTRUM\n",
        "print(\"finally start...\")\n",
        "specobj = Spectrogram(n_fft=6*hop, win_length=6*hop, hop_length=hop, pad=0, power=2, normalized=True)\n",
        "specfunc = specobj.forward\n",
        "melobj = MelScale(n_mels=hop, sample_rate=sr, f_min=0.)\n",
        "melfunc = melobj.forward\n",
        "\n",
        "def melspecfunc(waveform):\n",
        "  specgram = specfunc(waveform)\n",
        "  mel_specgram = melfunc(specgram)\n",
        "  return mel_specgram\n",
        "\n",
        "def spectral_convergence(input, target):\n",
        "    return 20 * ((input - target).norm().log10() - target.norm().log10())\n",
        "\n",
        "def GRAD(spec, transform_fn, samples=None, init_x0=None, maxiter=1000, tol=1e-6, verbose=1, evaiter=10, lr=0.003):\n",
        "\n",
        "    spec = torch.Tensor(spec)\n",
        "    samples = (spec.shape[-1]*hop)-hop\n",
        "\n",
        "    if init_x0 is None:\n",
        "        init_x0 = spec.new_empty((1,samples)).normal_(std=1e-6)\n",
        "    x = nn.Parameter(init_x0)\n",
        "    T = spec\n",
        "\n",
        "    criterion = nn.L1Loss()\n",
        "    optimizer = torch.optim.Adam([x], lr=lr)\n",
        "\n",
        "    bar_dict = {}\n",
        "    metric_func = spectral_convergence\n",
        "    bar_dict['spectral_convergence'] = 0\n",
        "    metric = 'spectral_convergence'\n",
        "\n",
        "    init_loss = None\n",
        "    with tqdm(total=maxiter, disable=not verbose) as pbar:\n",
        "        for i in range(maxiter):\n",
        "            optimizer.zero_grad()\n",
        "            V = transform_fn(x)\n",
        "            loss = criterion(V, T)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr = lr*0.9999\n",
        "            for param_group in optimizer.param_groups:\n",
        "              param_group['lr'] = lr\n",
        "\n",
        "            if i % evaiter == evaiter - 1:\n",
        "                with torch.no_grad():\n",
        "                    V = transform_fn(x)\n",
        "                    bar_dict[metric] = metric_func(V, spec).item()\n",
        "                    l2_loss = criterion(V, spec).item()\n",
        "                    pbar.set_postfix(**bar_dict, loss=l2_loss)\n",
        "                    pbar.update(evaiter)\n",
        "\n",
        "    return x.detach().view(-1).cpu()\n",
        "\n",
        "def normalize(S):\n",
        "  return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1)\n",
        "\n",
        "def denormalize(S):\n",
        "  return (((np.clip(S, -1, 1)+1.)/2.) * -min_level_db) + min_level_db\n",
        "\n",
        "def prep(wv,hop=192):\n",
        "  S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1,-1))).detach().cpu())\n",
        "  S = librosa.power_to_db(S)-ref_level_db\n",
        "  return normalize(S)\n",
        "\n",
        "def deprep(S):\n",
        "  S = denormalize(S)+ref_level_db\n",
        "  S = librosa.db_to_power(S)\n",
        "  wv = GRAD(np.expand_dims(S,0), melspecfunc, maxiter=2000, evaiter=10, tol=1e-8)\n",
        "  return np.array(np.squeeze(wv))\n",
        "\n",
        "#Helper functions\n",
        "\n",
        "#Generate spectrograms from waveform array\n",
        "def tospec(data):\n",
        "  specs=np.empty(data.shape[0], dtype=object)\n",
        "  for i in range(data.shape[0]):\n",
        "    x = data[i]\n",
        "    S=prep(x)\n",
        "    S = np.array(S, dtype=np.float32)\n",
        "    specs[i]=np.expand_dims(S, -1)\n",
        "  print(specs.shape)\n",
        "  return specs\n",
        "\n",
        "#Generate multiple spectrograms with a determined length from single wav file\n",
        "def tospeclong(path, length=4*16000):\n",
        "  x, sr = librosa.load(path,sr=16000)\n",
        "  x,_ = librosa.effects.trim(x)\n",
        "  loudls = librosa.effects.split(x, top_db=50)\n",
        "  xls = np.array([])\n",
        "  for interv in loudls:\n",
        "    xls = np.concatenate((xls,x[interv[0]:interv[1]]))\n",
        "  x = xls\n",
        "  num = x.shape[0]//length\n",
        "  specs=np.empty(num, dtype=object)\n",
        "  for i in range(num-1):\n",
        "    a = x[i*length:(i+1)*length]\n",
        "    S = prep(a)\n",
        "    S = np.array(S, dtype=np.float32)\n",
        "    try:\n",
        "      sh = S.shape\n",
        "      specs[i]=S\n",
        "    except AttributeError:\n",
        "      print('spectrogram failed')\n",
        "  print(specs.shape)\n",
        "  return specs\n",
        "\n",
        "!pip install progressbar\n",
        "import progressbar\n",
        "from time import sleep\n",
        "import sys\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "\n",
        "\n",
        "#Waveform array from path of folder containing wav files\n",
        "def audio_array(path, tag):\n",
        "  ls = glob(f'{path}/*.wav')\n",
        "  adata = []\n",
        "  bdata = []\n",
        "  print(\"reading is about to start..the length of the data is:\", len(ls))\n",
        "  out = display(progress(0, len(ls)), display_id=True)\n",
        "  \n",
        "  for i in range(len(ls)):\n",
        "    out.update(progress(i, len(ls)))\n",
        "\n",
        "    x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
        "    x = np.array(x, dtype=np.float32)\n",
        "    adata.append(x)\n",
        "    #check for the target value\n",
        "    if(ls[i].split(\"_\")[2]==tag):\n",
        "      y, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
        "      y = np.array(y, dtype=np.float32)\n",
        "      bdata.append(y)\n",
        "  return np.array(adata), np.array(bdata)\n",
        "\n",
        "#Concatenate spectrograms in array along the time axis\n",
        "def testass(a):\n",
        "  but=False\n",
        "  con = np.array([])\n",
        "  nim = a.shape[0]\n",
        "  for i in range(nim):\n",
        "    im = a[i]\n",
        "    im = np.squeeze(im)\n",
        "    if not but:\n",
        "      con=im\n",
        "      but=True\n",
        "    else:\n",
        "      con = np.concatenate((con,im), axis=1)\n",
        "  return np.squeeze(con)\n",
        "\n",
        "#Split spectrograms in chunks with equal size\n",
        "def splitcut(data):\n",
        "  ls = []\n",
        "  mini = 0\n",
        "  minifinal = 10*shape                                                              #max spectrogram length\n",
        "  for i in range(data.shape[0]-1):\n",
        "    if data[i].shape[1]<=data[i+1].shape[1]:\n",
        "      mini = data[i].shape[1]\n",
        "    else:\n",
        "      mini = data[i+1].shape[1]\n",
        "    if mini>=3*shape and mini<minifinal:\n",
        "      minifinal = mini\n",
        "  for i in range(data.shape[0]):\n",
        "    x = data[i]\n",
        "    if x.shape[1]>=3*shape:\n",
        "      for n in range(x.shape[1]//minifinal):\n",
        "        ls.append(x[:,n*minifinal:n*minifinal+minifinal,:])\n",
        "      ls.append(x[:,-minifinal:,:])\n",
        "  return np.array(ls)\n",
        "\n",
        "#Generating Mel-Spectrogram dataset (Uncomment where needed)\n",
        "#adata: source spectrograms\n",
        "#bdata: target spectrograms\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finally start...\n",
            "Collecting progressbar\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/a6/b8e451f6cff1c99b4747a2f7235aa904d2d49e8e1464e0b798272aa84358/progressbar-2.5.tar.gz\n",
            "Building wheels for collected packages: progressbar\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-cp36-none-any.whl size=12074 sha256=f6ce94db8f84f0f2a12d404dc84158e1d787359764d75c2903a87d682f7d44fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/e9/6b/ea01090205e285175842339aa3b491adeb4015206cda272ff0\n",
            "Successfully built progressbar\n",
            "Installing collected packages: progressbar\n",
            "Successfully installed progressbar-2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fQNIeJ76aPd"
      },
      "source": [
        "# Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "teZtRq706W6d",
        "outputId": "395850b3-e646-4e46-ff00-09aab145ecaa"
      },
      "source": [
        "#ALL BUT HAPPY\n",
        "print(\"start reading files\")\n",
        "awv, bwv = audio_array('/content/drive/My Drive/Data/AudioWAV', tag=tag)                            #get waveform array from folder containing wav files\n",
        "aspec = tospec(awv)                                                                 #get spectrogram array\n",
        "adata = splitcut(aspec)                                                             #split spectrogams to fixed length\n",
        "#HAPPY\n",
        "#bwv = audio_array('/content/drive/My Drive/Data/AudioWAV', \"HAP\")\n",
        "bspec = tospec(bwv)\n",
        "bdata = splitcut(bspec)\n",
        "\n",
        "print(\"Data is ready somehow :) \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start reading files\n",
            "reading is about to start..the length of the data is: 7443\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='7442'\n",
              "            max='7443',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            7442\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
            "  normalized, onesided, return_complex)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
            "  normalized, onesided, return_complex)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(7443,)\n",
            "(1271,)\n",
            "Data is ready somehow :) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCooRXTd7Czn"
      },
      "source": [
        "# configuring data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR_AeFUA7Kj-"
      },
      "source": [
        "\n",
        "\n",
        "#Creating Tensorflow Datasets\n",
        "\n",
        "@tf.function\n",
        "def proc(x):\n",
        "  return tf.image.random_crop(x, size=[hop, 3*shape, 1])\n",
        "\n",
        "dsa = tf.data.Dataset.from_tensor_slices(adata).repeat(10).map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)\n",
        "dsb = tf.data.Dataset.from_tensor_slices(bdata).repeat(20).map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)\n",
        "\n",
        "\n",
        "history_D=[]\n",
        "history_G=[]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4tWpzLX7Xcx"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy1F7v4Q7a6g"
      },
      "source": [
        "# Adding Spectral Normalization to convolutional layers\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import sparse_ops\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "from tensorflow.python.ops import standard_ops\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (tf.norm(v) + eps)\n",
        "\n",
        "\n",
        "class ConvSN2D(tf.keras.layers.Conv2D):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
        "        super(ConvSN2D, self).__init__(filters, kernel_size, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ConvSN2D, self).build(input_shape)\n",
        "\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "\n",
        "        self.u = self.add_weight(self.name + '_u',\n",
        "                                 shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                                 initializer=tf.initializers.RandomNormal(0, 1),\n",
        "                                 trainable=False\n",
        "                                 )\n",
        "\n",
        "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
        "        for _ in range(self.power_iterations):\n",
        "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
        "            new_u = l2normalize(tf.matmul(new_v, W))\n",
        "\n",
        "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
        "        W_bar = W / sigma\n",
        "\n",
        "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
        "            W_bar = tf.reshape(W_bar, W_shape)\n",
        "\n",
        "        return W_bar\n",
        "\n",
        "    def call(self, inputs):\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
        "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
        "        outputs = self._convolution_op(inputs, new_kernel)\n",
        "\n",
        "        if self.use_bias:\n",
        "            if self.data_format == 'channels_first':\n",
        "                outputs = tf.nn.bias_add(outputs, self.bias, data_format='NCHW')\n",
        "            else:\n",
        "                outputs = tf.nn.bias_add(outputs, self.bias, data_format='NHWC')\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class ConvSN2DTranspose(tf.keras.layers.Conv2DTranspose):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
        "        super(ConvSN2DTranspose, self).__init__(filters, kernel_size, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ConvSN2DTranspose, self).build(input_shape)\n",
        "\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "\n",
        "        self.u = self.add_weight(self.name + '_u',\n",
        "                                 shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                                 initializer=tf.initializers.RandomNormal(0, 1),\n",
        "                                 trainable=False\n",
        "                                 )\n",
        "\n",
        "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
        "        for _ in range(self.power_iterations):\n",
        "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
        "            new_u = l2normalize(tf.matmul(new_v, W))\n",
        "\n",
        "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
        "        W_bar = W / sigma\n",
        "\n",
        "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
        "            W_bar = tf.reshape(W_bar, W_shape)\n",
        "\n",
        "        return W_bar\n",
        "\n",
        "    def call(self, inputs):\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
        "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
        "\n",
        "        inputs_shape = array_ops.shape(inputs)\n",
        "        batch_size = inputs_shape[0]\n",
        "        if self.data_format == 'channels_first':\n",
        "            h_axis, w_axis = 2, 3\n",
        "        else:\n",
        "            h_axis, w_axis = 1, 2\n",
        "\n",
        "        height, width = inputs_shape[h_axis], inputs_shape[w_axis]\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        stride_h, stride_w = self.strides\n",
        "\n",
        "        if self.output_padding is None:\n",
        "            out_pad_h = out_pad_w = None\n",
        "        else:\n",
        "            out_pad_h, out_pad_w = self.output_padding\n",
        "\n",
        "        out_height = conv_utils.deconv_output_length(height,\n",
        "                                                     kernel_h,\n",
        "                                                     padding=self.padding,\n",
        "                                                     output_padding=out_pad_h,\n",
        "                                                     stride=stride_h,\n",
        "                                                     dilation=self.dilation_rate[0])\n",
        "        out_width = conv_utils.deconv_output_length(width,\n",
        "                                                    kernel_w,\n",
        "                                                    padding=self.padding,\n",
        "                                                    output_padding=out_pad_w,\n",
        "                                                    stride=stride_w,\n",
        "                                                    dilation=self.dilation_rate[1])\n",
        "        if self.data_format == 'channels_first':\n",
        "            output_shape = (batch_size, self.filters, out_height, out_width)\n",
        "        else:\n",
        "            output_shape = (batch_size, out_height, out_width, self.filters)\n",
        "\n",
        "        output_shape_tensor = array_ops.stack(output_shape)\n",
        "        outputs = K.conv2d_transpose(\n",
        "            inputs,\n",
        "            new_kernel,\n",
        "            output_shape_tensor,\n",
        "            strides=self.strides,\n",
        "            padding=self.padding,\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate)\n",
        "\n",
        "        if not context.executing_eagerly():\n",
        "            out_shape = self.compute_output_shape(inputs.shape)\n",
        "            outputs.set_shape(out_shape)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = tf.nn.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DenseSN(Dense):\n",
        "    def build(self, input_shape):\n",
        "        super(DenseSN, self).build(input_shape)\n",
        "\n",
        "        self.u = self.add_weight(self.name + '_u',\n",
        "                                 shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                                 initializer=tf.initializers.RandomNormal(0, 1),\n",
        "                                 trainable=False)\n",
        "\n",
        "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
        "        new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
        "        new_u = l2normalize(tf.matmul(new_v, W))\n",
        "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
        "        W_bar = W / sigma\n",
        "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
        "            W_bar = tf.reshape(W_bar, W_shape)\n",
        "        return W_bar\n",
        "\n",
        "    def call(self, inputs):\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
        "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
        "        rank = len(inputs.shape)\n",
        "        if rank > 2:\n",
        "            outputs = standard_ops.tensordot(inputs, new_kernel, [[rank - 1], [0]])\n",
        "            if not context.executing_eagerly():\n",
        "                shape = inputs.shape.as_list()\n",
        "                output_shape = shape[:-1] + [self.units]\n",
        "                outputs.set_shape(output_shape)\n",
        "        else:\n",
        "            inputs = math_ops.cast(inputs, self._compute_dtype)\n",
        "            if K.is_sparse(inputs):\n",
        "                outputs = sparse_ops.sparse_tensor_dense_matmul(inputs, new_kernel)\n",
        "            else:\n",
        "                outputs = gen_math_ops.mat_mul(inputs, new_kernel)\n",
        "        if self.use_bias:\n",
        "            outputs = tf.nn.bias_add(outputs, self.bias)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "#Networks Architecture\n",
        "\n",
        "init = tf.keras.initializers.he_uniform()\n",
        "\n",
        "def conv2d(layer_input, filters, kernel_size=4, strides=2, padding='same', leaky=True, bnorm=True, sn=True):\n",
        "  if leaky:\n",
        "    Activ = LeakyReLU(alpha=0.2)\n",
        "  else:\n",
        "    Activ = ReLU()\n",
        "  if sn:\n",
        "    d = ConvSN2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)\n",
        "  else:\n",
        "    d = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)\n",
        "  if bnorm:\n",
        "    d = BatchNormalization()(d)\n",
        "  d = Activ(d)\n",
        "  return d\n",
        "\n",
        "def deconv2d(layer_input, layer_res, filters, kernel_size=4, conc=True, scalev=False, bnorm=True, up=True, padding='same', strides=2):\n",
        "  if up:\n",
        "    u = UpSampling2D((1,2))(layer_input)\n",
        "    u = ConvSN2D(filters, kernel_size, strides=(1,1), kernel_initializer=init, use_bias=False, padding=padding)(u)\n",
        "  else:\n",
        "    u = ConvSN2DTranspose(filters, kernel_size, strides=strides, kernel_initializer=init, use_bias=False, padding=padding)(layer_input)\n",
        "  if bnorm:\n",
        "    u = BatchNormalization()(u)\n",
        "  u = LeakyReLU(alpha=0.2)(u)\n",
        "  if conc:\n",
        "    u = Concatenate()([u,layer_res])\n",
        "  return u\n",
        "\n",
        "#Extract function: splitting spectrograms\n",
        "def extract_image(im):\n",
        "  im1 = Cropping2D(((0,0), (0, 2*(im.shape[2]//3))))(im)\n",
        "  im2 = Cropping2D(((0,0), (im.shape[2]//3,im.shape[2]//3)))(im)\n",
        "  im3 = Cropping2D(((0,0), (2*(im.shape[2]//3), 0)))(im)\n",
        "  return im1,im2,im3\n",
        "\n",
        "#Assemble function: concatenating spectrograms\n",
        "def assemble_image(lsim):\n",
        "  im1,im2,im3 = lsim\n",
        "  imh = Concatenate(2)([im1,im2,im3])\n",
        "  return imh\n",
        "\n",
        "#U-NET style architecture\n",
        "def build_generator(input_shape):\n",
        "  h,w,c = input_shape\n",
        "  inp = Input(shape=input_shape)\n",
        "  #downscaling\n",
        "  g0 = tf.keras.layers.ZeroPadding2D((0,1))(inp)\n",
        "  g1 = conv2d(g0, 256, kernel_size=(h,3), strides=1, padding='valid')\n",
        "  g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2))\n",
        "  g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2))\n",
        "  #upscaling\n",
        "  g4 = deconv2d(g3,g2, 256, kernel_size=(1,7), strides=(1,2))\n",
        "  g5 = deconv2d(g4,g1, 256, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
        "  g6 = ConvSN2DTranspose(1, kernel_size=(h,1), strides=(1,1), kernel_initializer=init, padding='valid', activation='tanh')(g5)\n",
        "  return Model(inp,g6, name='G')\n",
        "\n",
        "#Siamese Network\n",
        "def build_siamese(input_shape):\n",
        "  h,w,c = input_shape\n",
        "  inp = Input(shape=input_shape)\n",
        "  g1 = conv2d(inp, 256, kernel_size=(h,3), strides=1, padding='valid', sn=False)\n",
        "  g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2), sn=False)\n",
        "  g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2), sn=False)\n",
        "  g4 = Flatten()(g3)\n",
        "  g5 = Dense(vec_len)(g4)\n",
        "  return Model(inp, g5, name='S')\n",
        "\n",
        "#Discriminator (Critic) Network\n",
        "def build_critic(input_shape):\n",
        "  h,w,c = input_shape\n",
        "  inp = Input(shape=input_shape)\n",
        "  g1 = conv2d(inp, 512, kernel_size=(h,3), strides=1, padding='valid', bnorm=False)\n",
        "  g2 = conv2d(g1, 512, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
        "  g3 = conv2d(g2, 512, kernel_size=(1,7), strides=(1,2), bnorm=False)\n",
        "  g4 = Flatten()(g3)\n",
        "  g4 = DenseSN(1, kernel_initializer=init)(g4)\n",
        "  return Model(inp, g4, name='C')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Load past models from path to resume training or test\n",
        "def load(path):\n",
        "  gen = build_generator((hop,shape,1))\n",
        "  siam = build_siamese((hop,shape,1))\n",
        "  critic = build_critic((hop,3*shape,1))\n",
        "  gen.load_weights(path+'/gen.h5')\n",
        "  critic.load_weights(path+'/critic.h5')\n",
        "  siam.load_weights(path+'/siam.h5')\n",
        "  return gen,critic,siam\n",
        "\n",
        "#Build models\n",
        "def build():\n",
        "  gen = build_generator((hop,shape,1))\n",
        "  siam = build_siamese((hop,shape,1))\n",
        "  critic = build_critic((hop,3*shape,1))                                          #the discriminator accepts as input spectrograms of triple the width of those generated by the generator\n",
        "  return gen,critic,siam\n",
        "\n",
        "#Generate a random batch to display current training results\n",
        "def testgena():\n",
        "  sw = True\n",
        "  while sw:\n",
        "    a = np.random.choice(aspec)\n",
        "    if a.shape[1]//shape!=1:\n",
        "      sw=False\n",
        "  dsa = []\n",
        "  if a.shape[1]//shape>6:\n",
        "    num=6\n",
        "  else:\n",
        "    num=a.shape[1]//shape\n",
        "  rn = np.random.randint(a.shape[1]-(num*shape))\n",
        "  for i in range(num):\n",
        "    im = a[:,rn+(i*shape):rn+(i*shape)+shape]\n",
        "    im = np.reshape(im, (im.shape[0],im.shape[1],1))\n",
        "    dsa.append(im)\n",
        "  return np.array(dsa, dtype=np.float32)\n",
        "\n",
        "#Show results mid-training\n",
        "def save_test_image_full(path):\n",
        "  a = testgena()\n",
        "  print(a.shape)\n",
        "  ab = gen(a, training=False)\n",
        "  ab = testass(ab)\n",
        "  a = testass(a)\n",
        "  abwv = deprep(ab)\n",
        "  awv = deprep(a)\n",
        "  sf.write(path+'/new_file.wav', abwv, sr)\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
        "  fig, axs = plt.subplots(ncols=2)\n",
        "  axs[0].imshow(np.flip(a, -2), cmap=None)\n",
        "  axs[0].axis('off')\n",
        "  axs[0].set_title('Source')\n",
        "  axs[1].imshow(np.flip(ab, -2), cmap=None)\n",
        "  axs[1].axis('off')\n",
        "  axs[1].set_title('Generated')\n",
        "  plt.show()\n",
        "\n",
        "#Save in training loop\n",
        "def save_end(epoch,gloss,closs,mloss,n_save=3,save_path='/content/drive/My Drive/ML_project/SAD'):                 #use custom save_path (i.e. Drive '../content/drive/My Drive/')\n",
        "  if epoch % n_save == 0:\n",
        "    print('Saving...')\n",
        "    path = f'{save_path}/MELGANVC-{tag}-{str(epoch)}'\n",
        "    os.mkdir(path)\n",
        "    gen.save_weights(path+'/gen.h5')\n",
        "    critic.save_weights(path+'/critic.h5')\n",
        "    siam.save_weights(path+'/siam.h5')\n",
        "    save_test_image_full(path)\n",
        "\n",
        "\n",
        "#Losses\n",
        "\n",
        "def mae(x,y):\n",
        "  return tf.reduce_mean(tf.abs(x-y))\n",
        "\n",
        "def mse(x,y):\n",
        "  return tf.reduce_mean((x-y)**2)\n",
        "\n",
        "def loss_travel(sa,sab,sa1,sab1):\n",
        "  l1 = tf.reduce_mean(((sa-sa1) - (sab-sab1))**2)\n",
        "  l2 = tf.reduce_mean(tf.reduce_sum(-(tf.nn.l2_normalize(sa-sa1, axis=[-1]) * tf.nn.l2_normalize(sab-sab1, axis=[-1])), axis=-1))\n",
        "  return l1+l2\n",
        "\n",
        "def loss_siamese(sa,sa1):\n",
        "  logits = tf.sqrt(tf.reduce_sum((sa-sa1)**2, axis=-1, keepdims=True))\n",
        "  return tf.reduce_mean(tf.square(tf.maximum((delta - logits), 0)))\n",
        "\n",
        "def d_loss_f(fake):\n",
        "  return tf.reduce_mean(tf.maximum(1 + fake, 0))\n",
        "\n",
        "def d_loss_r(real):\n",
        "  return tf.reduce_mean(tf.maximum(1 - real, 0))\n",
        "\n",
        "def g_loss_f(fake):\n",
        "  return tf.reduce_mean(- fake)\n",
        "\n",
        "#Get models and optimizers\n",
        "def get_networks(shape, load_model=False, path=None):\n",
        "  if not load_model:\n",
        "    gen,critic,siam = build()\n",
        "  else:\n",
        "    gen,critic,siam = load(path)\n",
        "  print('Built networks')\n",
        "\n",
        "  opt_gen = Adam(0.0001, 0.5)\n",
        "  opt_disc = Adam(0.0001, 0.5)\n",
        "\n",
        "  return gen,critic,siam, [opt_gen,opt_disc]\n",
        "\n",
        "#Set learning rate\n",
        "def update_lr(lr):\n",
        "  opt_gen.learning_rate = lr\n",
        "  opt_disc.learning_rate = lr\n",
        "\n",
        "\n",
        "# Training Functions\n",
        "\n",
        "# Train Generator, Siamese and Critic\n",
        "@tf.function\n",
        "def train_all(a, b):\n",
        "    # splitting spectrogram in 3 parts\n",
        "    aa, aa2, aa3 = extract_image(a)\n",
        "    bb, bb2, bb3 = extract_image(b)\n",
        "\n",
        "    with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_disc:\n",
        "        # translating A to B\n",
        "        fab = gen(aa, training=True)\n",
        "        fab2 = gen(aa2, training=True)\n",
        "        fab3 = gen(aa3, training=True)\n",
        "        # identity mapping B to B                                                        COMMENT THESE 3 LINES IF THE IDENTITY LOSS TERM IS NOT NEEDED\n",
        "        fid = gen(bb, training=True)\n",
        "        fid2 = gen(bb2, training=True)\n",
        "        fid3 = gen(bb3, training=True)\n",
        "        # concatenate/assemble converted spectrograms\n",
        "        fabtot = assemble_image([fab, fab2, fab3])\n",
        "\n",
        "        # feed concatenated spectrograms to critic\n",
        "        cab = critic(fabtot, training=True)\n",
        "        cb = critic(b, training=True)\n",
        "        # feed 2 pairs (A,G(A)) extracted spectrograms to Siamese\n",
        "        sab = siam(fab, training=True)\n",
        "        sab2 = siam(fab3, training=True)\n",
        "        sa = siam(aa, training=True)\n",
        "        sa2 = siam(aa3, training=True)\n",
        "\n",
        "        # identity mapping loss\n",
        "        loss_id = (mae(bb, fid) + mae(bb2, fid2) + mae(bb3,\n",
        "                                                       fid3)) / 3.  # loss_id = 0. IF THE IDENTITY LOSS TERM IS NOT NEEDED\n",
        "        # travel loss\n",
        "        loss_m = loss_travel(sa, sab, sa2, sab2) + loss_siamese(sa, sa2)\n",
        "        # generator and critic losses\n",
        "        loss_g = g_loss_f(cab)\n",
        "        loss_dr = d_loss_r(cb)\n",
        "        loss_df = d_loss_f(cab)\n",
        "        loss_d = (loss_dr + loss_df) / 2.\n",
        "        # generator+siamese total loss\n",
        "        lossgtot = loss_g + 10. * loss_m + 0.5 * loss_id  # CHANGE LOSS WEIGHTS HERE  (COMMENT OUT +w*loss_id IF THE IDENTITY LOSS TERM IS NOT NEEDED)\n",
        "\n",
        "    # computing and applying gradients\n",
        "    grad_gen = tape_gen.gradient(lossgtot, gen.trainable_variables + siam.trainable_variables)\n",
        "    opt_gen.apply_gradients(zip(grad_gen, gen.trainable_variables + siam.trainable_variables))\n",
        "\n",
        "    grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)\n",
        "    opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))\n",
        "\n",
        "    return loss_dr, loss_df, loss_g, loss_id\n",
        "\n",
        "\n",
        "# Train Critic only\n",
        "@tf.function\n",
        "def train_d(a, b):\n",
        "    aa, aa2, aa3 = extract_image(a)\n",
        "    with tf.GradientTape() as tape_disc:\n",
        "        fab = gen(aa, training=True)\n",
        "        fab2 = gen(aa2, training=True)\n",
        "        fab3 = gen(aa3, training=True)\n",
        "        fabtot = assemble_image([fab, fab2, fab3])\n",
        "\n",
        "        cab = critic(fabtot, training=True)\n",
        "        cb = critic(b, training=True)\n",
        "\n",
        "        loss_dr = d_loss_r(cb)\n",
        "        loss_df = d_loss_f(cab)\n",
        "\n",
        "        loss_d = (loss_dr + loss_df) / 2.\n",
        "\n",
        "    grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)\n",
        "    opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))\n",
        "\n",
        "    return loss_dr, loss_df\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "history=[]\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "def train(epochs, batch_size=16, lr=0.0001, n_save=6, gupt=5):\n",
        "    update_lr(lr)\n",
        "    df_list = []\n",
        "    dr_list = []\n",
        "    g_list = []\n",
        "    id_list = []\n",
        "    c = 0\n",
        "    g = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        bef = time.time()\n",
        "\n",
        "        for batchi, (a, b) in enumerate(zip(dsa, dsb)):\n",
        "\n",
        "            if batchi % gupt == 0:\n",
        "                dloss_t, dloss_f, gloss, idloss = train_all(a, b)\n",
        "            else:\n",
        "                dloss_t, dloss_f = train_d(a, b)\n",
        "\n",
        "            \n",
        "\n",
        "            df_list.append(dloss_f)\n",
        "            dr_list.append(dloss_t)\n",
        "            g_list.append(gloss)\n",
        "            id_list.append(idloss)\n",
        "            c += 1\n",
        "            g += 1\n",
        "\n",
        "            if batchi % 1000 == 0:\n",
        "                print(f'[Epoch {epoch}/{epochs}] [Batch {batchi}] [D loss f: {np.mean(df_list[-g:], axis=0)} ', end='')\n",
        "                print(f'r: {np.mean(dr_list[-g:], axis=0)}] ', end='')\n",
        "                print(f'[G loss: {np.mean(g_list[-g:], axis=0)}] ', end='')\n",
        "                print(f'[ID loss: {np.mean(id_list[-g:])}] ', end='')\n",
        "                print(f'[LR: {lr}]')\n",
        "                g = 0\n",
        "            nbatch = batchi\n",
        "\n",
        "\n",
        "        history_D.append(np.mean(df_list[-c:], axis=0))\n",
        "        history_G.append(np.mean(g_list[-c:], axis=0))\n",
        "        print(history_D,\"the next is \", history_G)\n",
        "        print(f'Time/Batch {(time.time() - bef) / nbatch}')\n",
        "        save_end(epoch, np.mean(g_list[-n_save * c:], axis=0), np.mean(df_list[-n_save * c:], axis=0),\n",
        "                 np.mean(id_list[-n_save * c:], axis=0), n_save=n_save)\n",
        "        print(\n",
        "            f'Mean D loss: {np.mean(df_list[-c:], axis=0)} Mean G loss: {np.mean(g_list[-c:], axis=0)} Mean ID loss: {np.mean(id_list[-c:], axis=0)}')\n",
        "        c = 0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfCgyF5C7cLV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "OIdC5BlT7fs-",
        "outputId": "821f8f1f-f863-4e72-b60c-5dcd885948a9"
      },
      "source": [
        "print(\"ready for definition of the model\")\n",
        "#/content/drive/MyDrive/ML_project/best_first/critic.h5\n",
        "gen,critic,siam, [opt_gen,opt_disc] = get_networks(shape, load_model=False, path='../content/drive/MyDrive/ML_project/best_sad')\n",
        "\n",
        "#Training\n",
        "\n",
        "#n_save = how many epochs between each saving and displaying of results\n",
        "#gupt = how many discriminator updates for generator+siamese update\n",
        "\n",
        "\n",
        "print(\"ready to start\")\n",
        "input(\"just say something to start LOL\")\n",
        "train(5, batch_size=bs, lr=0.0002, n_save=1, gupt=3)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ready for definition of the model\n",
            "Built networks\n",
            "ready to start\n",
            "just say something to start LOLgo\n",
            "[Epoch 0/5] [Batch 0] [D loss f: 0.9766133427619934 r: 1.0500550270080566] [G loss: 0.02338663674890995] [ID loss: 0.43830999732017517] [LR: 0.0002]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-77ca164be5c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ready to start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"just say something to start LOL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgupt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-331c7aee9c37>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, lr, n_save, gupt)\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0mdloss_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdloss_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                 \u001b[0mdloss_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m                 ))\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/tmpl1essvgp.py\u001b[0m in \u001b[0;36mtf__train_d\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maa2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maa3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape_disc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0mfab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0mfab2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mfab3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whitelisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \"\"\"\n\u001b[1;32m    385\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 386\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/tmpsmpusbah.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mW_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mW_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mnew_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_spectral_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0m_attach_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/tmpftsdy1ck.py\u001b[0m in \u001b[0;36mtf__compute_spectral_norm\u001b[0;34m(self, W, new_u, W_shape)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'new_u'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'new_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mW_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whitelisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose_v2\u001b[0;34m(a, perm, conjugate, name)\u001b[0m\n\u001b[1;32m   2105\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m   \"\"\"\n\u001b[0;32m-> 2107\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconjugate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name, conjugate)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m       \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranspose_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m  11533\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11534\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m> 11535\u001b[0;31m         \"Transpose\", x=x, perm=perm, name=name)\n\u001b[0m\u001b[1;32m  11536\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11537\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    591\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3483\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3485\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3486\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1923\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m       if not all(\n\u001b[0;32m-> 1925\u001b[0;31m           x.is_compatible_with(i.dtype) for i, x in zip(inputs, input_types)):\n\u001b[0m\u001b[1;32m   1926\u001b[0m         raise TypeError(\"In op '%s', input types (%s) are not compatible \"\n\u001b[1;32m   1927\u001b[0m                         \u001b[0;34m\"with expected types (%s)\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1923\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m       if not all(\n\u001b[0;32m-> 1925\u001b[0;31m           x.is_compatible_with(i.dtype) for i, x in zip(inputs, input_types)):\n\u001b[0m\u001b[1;32m   1926\u001b[0m         raise TypeError(\"In op '%s', input types (%s) are not compatible \"\n\u001b[1;32m   1927\u001b[0m                         \u001b[0;34m\"with expected types (%s)\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vqlFv367plI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2506d2f2-1378-4af5-c6e5-b2f3f5803d39"
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "t = np.linspace(0, len(history_D)*10, len(history_D))\n",
        "\n",
        "plt.plot(t, history_D,  label='Discriminator') # plotting t, a separately \n",
        "plt.plot(t, history_G,  label='Generator') # plotting t, b separately\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('loss') \n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE5-snIR75Uj"
      },
      "source": [
        "# after training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGnwC_9A78a6"
      },
      "source": [
        "#Assembling generated Spectrogram chunks into final Spectrogram\n",
        "def specass(a,spec):\n",
        "  but=False\n",
        "  con = np.array([])\n",
        "  nim = a.shape[0]\n",
        "  for i in range(nim-1):\n",
        "    im = a[i]\n",
        "    im = np.squeeze(im)\n",
        "    if not but:\n",
        "      con=im\n",
        "      but=True\n",
        "    else:\n",
        "      con = np.concatenate((con,im), axis=1)\n",
        "  diff = spec.shape[1]-(nim*shape)\n",
        "  a = np.squeeze(a)\n",
        "  con = np.concatenate((con,a[-1,:,-diff:]), axis=1)\n",
        "  return np.squeeze(con)\n",
        "\n",
        "#Splitting input spectrogram into different chunks to feed to the generator\n",
        "def chopspec(spec):\n",
        "  dsa=[]\n",
        "  for i in range(spec.shape[1]//shape):\n",
        "    im = spec[:,i*shape:i*shape+shape]\n",
        "    im = np.reshape(im, (im.shape[0],im.shape[1],1))\n",
        "    dsa.append(im)\n",
        "  imlast = spec[:,-shape:]\n",
        "  imlast = np.reshape(imlast, (imlast.shape[0],imlast.shape[1],1))\n",
        "  dsa.append(imlast)\n",
        "  return np.array(dsa, dtype=np.float32)\n",
        "\n",
        "#Converting from source Spectrogram to target Spectrogram\n",
        "def towave(spec, name, path='../content/', show=False):\n",
        "  specarr = chopspec(spec)\n",
        "  print(specarr.shape)\n",
        "  a = specarr\n",
        "  print('Generating...')\n",
        "  ab = gen(a, training=False)\n",
        "  print('Assembling and Converting...')\n",
        "  a = specass(a,spec)\n",
        "  ab = specass(ab,spec)\n",
        "  awv = deprep(a)\n",
        "  abwv = deprep(ab)\n",
        "  print('Saving...')\n",
        "  pathfin = f'{path}/{name}'\n",
        "  os.mkdir(pathfin)\n",
        "  sf.write(pathfin+'/AB.wav', abwv, sr)\n",
        "  sf.write(pathfin+'/A.wav', awv, sr)\n",
        "  print('Saved WAV!')\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
        "  if show:\n",
        "    fig, axs = plt.subplots(ncols=2)\n",
        "    axs[0].imshow(np.flip(a, -2), cmap=None)\n",
        "    axs[0].axis('off')\n",
        "    axs[0].set_title('Source')\n",
        "    axs[1].imshow(np.flip(ab, -2), cmap=None)\n",
        "    axs[1].axis('off')\n",
        "    axs[1].set_title('Generated')\n",
        "    plt.show()\n",
        "  return abwv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW17lKjH8AXK"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBqCHFKZ8CQB"
      },
      "source": [
        "!pip install gTTS\n",
        "!pip install pydub\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "\n",
        "tts = gTTS('this is the first test')\n",
        "test_path='../content/hello.mp3'\n",
        "tts.save(test_path)\n",
        "test_path1='../content/hello.wav'\n",
        "sound = AudioSegment.from_mp3(\"../content/hello.mp3\")\n",
        "sound.export(\"../content/hello.wav\", format=\"wav\")\n",
        "x, sr = tf.audio.decode_wav(tf.io.read_file(test_path1))\n",
        "x = np.array(x, dtype=np.float32)\n",
        "\n",
        "\n",
        "wv, sr = librosa.load(\"test3.wav\", sr=16000)  #Load source waveform \n",
        "print(wv.shape)\n",
        "speca = prep(wv)                                                    #Waveform to Spectrogram\n",
        "\n",
        "plt.figure(figsize=(50,1))                                          #Show Spectrogram\n",
        "plt.imshow(np.flip(speca, axis=0), cmap=None)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "abwv = towave(speca, name='FILENAME1', path='../content/')           #Convert and save wav\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}